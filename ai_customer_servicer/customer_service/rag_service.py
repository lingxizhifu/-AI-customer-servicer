"""
ç¦»çº¿ç‰ˆRAGæœåŠ¡ - æ— éœ€å¤–éƒ¨æ¨¡å‹ä¾èµ–
ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•ï¼Œé¿å…ç½‘ç»œè¿æ¥é—®é¢˜
"""
import os
import json
import re
import jieba
import sqlite3
import hashlib
from typing import List, Dict, Tuple
from datetime import datetime
from collections import Counter
import math
import pymysql
import importlib.util

MYSQL_CONFIG = {
    'host': 'localhost',
    'port': 3306,
    'user': 'root',
    'password': '123456',
    'database': 'ai_chat_db',
    'charset': 'utf8mb4'
}

def get_db_config():
    settings_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'ai_customer_servicer', 'settings.py')
    spec = importlib.util.spec_from_file_location('settings', settings_path)
    settings = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(settings)
    db = settings.DATABASES['default']
    return {
        'host': db.get('HOST', 'localhost'),
        'port': int(db.get('PORT', 3306)),
        'user': db.get('USER', 'root'),
        'password': db.get('PASSWORD', '123456'),
        'database': db.get('NAME', 'ai_chat_db'),
        'charset': db.get('OPTIONS', {}).get('charset', 'utf8mb4')
    }

class OfflineRAGService:
    """ç¦»çº¿RAGæœåŠ¡ - ä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹"""
    
    def __init__(self):
        self.init_database()
        print("âœ… ç¦»çº¿RAGæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    
    def init_database(self):
        """åˆå§‹åŒ–MySQLæ•°æ®åº“è¿æ¥"""
        self.conn = pymysql.connect(**MYSQL_CONFIG)
        print("âœ… MySQLæ•°æ®åº“è¿æ¥æˆåŠŸ")
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not text:
            return ""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
        text = re.sub(r'\s+', ' ', text.strip())
        return text
    
    def tokenize_chinese(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        try:
            # ä½¿ç”¨jiebaåˆ†è¯
            words = list(jieba.cut(text))
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'æœ‰', 'æ— ', 'ä¸', 'ä¹Ÿ', 'éƒ½', 'å¾ˆ', 'å°±', 'è¦', 'å¯ä»¥', 'èƒ½å¤Ÿ', 'ä¸€ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'æ—¶å€™'}
            filtered_words = [w.strip() for w in words if len(w.strip()) > 1 and w.strip() not in stop_words]
            return filtered_words
        except:
            # å¦‚æœjiebaå‡ºé—®é¢˜ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²
            return text.split()
    
    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ - ä½¿ç”¨TF-IDFä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            # åˆ†è¯
            tokens1 = self.tokenize_chinese(text1)
            tokens2 = self.tokenize_chinese(text2)
            
            if not tokens1 or not tokens2:
                return 0.0
            
            # è®¡ç®—è¯é¢‘
            tf1 = Counter(tokens1)
            tf2 = Counter(tokens2)
            
            # è·å–æ‰€æœ‰å”¯ä¸€è¯æ±‡
            all_words = set(tokens1 + tokens2)
            
            # è®¡ç®—TF-IDFå‘é‡
            def get_tf_idf_vector(tf_dict, all_words_set, total_docs=1000):
                vector = []
                total_words = sum(tf_dict.values())
                for word in all_words_set:
                    tf = tf_dict.get(word, 0) / total_words if total_words > 0 else 0
                    # ç®€åŒ–çš„IDFè®¡ç®—
                    idf = math.log(total_docs / (1 + tf_dict.get(word, 0)))
                    vector.append(tf * idf)
                return vector
            
            vector1 = get_tf_idf_vector(tf1, all_words)
            vector2 = get_tf_idf_vector(tf2, all_words)
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = sum(a * b for a, b in zip(vector1, vector2))
            magnitude1 = math.sqrt(sum(a * a for a in vector1))
            magnitude2 = math.sqrt(sum(b * b for b in vector2))
            
            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0
            
            similarity = dot_product / (magnitude1 * magnitude2)
            
            # é¢å¤–çš„å…³é”®è¯åŒ¹é…åŠ æƒ
            common_words = set(tokens1) & set(tokens2)
            keyword_bonus = len(common_words) / max(len(set(tokens1)), len(set(tokens2))) * 0.3
            
            return min(similarity + keyword_bonus, 1.0)
            
        except Exception as e:
            print(f"âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å‡ºé”™: {e}")
            # é™çº§åˆ°ç®€å•å­—ç¬¦ä¸²åŒ¹é…
            return self.simple_string_similarity(text1, text2)
    
    def simple_string_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        text1 = text1.lower()
        text2 = text2.lower()
        
        # è®¡ç®—å…±åŒå­—ç¬¦
        common_chars = 0
        for char in text1:
            if char in text2:
                common_chars += 1
        
        max_len = max(len(text1), len(text2))
        if max_len == 0:
            return 0.0
        
        return common_chars / max_len
    
    def categorize_question(self, question: str) -> str:
        """é—®é¢˜åˆ†ç±»"""
        categories = {
            'åº“å­˜æŸ¥è¯¢': ['è´§', 'åº“å­˜', 'æœ‰æ²¡æœ‰', 'ç°è´§', 'æœ‰è´§', 'ç¼ºè´§', 'æ–­è´§'],
            'ç‰©æµé…é€': ['å‘è´§', 'å¿«é€’', 'ç‰©æµ', 'åˆ°è´§', 'é…é€', 'ä»€ä¹ˆæ—¶å€™', 'å¤šä¹…', 'è¿è´¹', 'é‚®è´¹'],
            'ä»·æ ¼å’¨è¯¢': ['ä¾¿å®œ', 'ä»·æ ¼', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 'å¤šå°‘é’±', 'è´¹ç”¨', 'æˆæœ¬', 'æ´»åŠ¨'],
            'äº§å“è´¨é‡': ['æ­£å“', 'è´¨é‡', 'çœŸå‡', 'å“è´¨', 'å¥½ç”¨', 'æ•ˆæœ', 'æ€ä¹ˆæ ·'],
            'å”®åæœåŠ¡': ['é€€è´§', 'æ¢è´§', 'é€€æ¬¾', 'å”®å', 'ç»´ä¿®', 'ä¸æ»¡æ„', 'é—®é¢˜', 'åäº†'],
            'è´¦æˆ·æœåŠ¡': ['ä¼šå‘˜', 'ç§¯åˆ†', 'ä¼˜æƒ åˆ¸', 'VIP', 'è´¦æˆ·', 'ç™»å½•', 'æ³¨å†Œ'],
            'è®¢å•ç®¡ç†': ['è®¢å•', 'å–æ¶ˆ', 'ä¿®æ”¹', 'æŸ¥è¯¢', 'çŠ¶æ€', 'æ”¯ä»˜'],
            'äº§å“å’¨è¯¢': ['æˆåˆ†', 'ä½¿ç”¨', 'æ•ˆæœ', 'è¯´æ˜ä¹¦', 'è§„æ ¼', 'å‚æ•°', 'åŠŸèƒ½'],
            'åº—é“ºæœåŠ¡': ['è¥ä¸šæ—¶é—´', 'åœ°å€', 'è”ç³»æ–¹å¼', 'å®¢æœ', 'ç”µè¯']
        }
        
        question_lower = question.lower()
        scores = {}
        
        for category, keywords in categories.items():
            score = sum(1 for keyword in keywords if keyword in question_lower)
            if score > 0:
                scores[category] = score
        
        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]
        
        return 'å…¶ä»–å’¨è¯¢'
    
    def generate_id(self, text: str) -> str:
        """ç”Ÿæˆå”¯ä¸€ID"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()[:16]
    
    def load_qa_data(self, qa_data: List[Dict]) -> Dict:
        """åŠ è½½é—®ç­”æ•°æ®åˆ°MySQLï¼ˆå¯é€‰ï¼Œé€šå¸¸ç”¨å¯¼å…¥å·¥å…·ï¼‰"""
        if not qa_data:
            return {'success': 0, 'error': 0}
        print(f"ğŸ”„ å¼€å§‹åŠ è½½ {len(qa_data)} æ¡é—®ç­”æ•°æ®...")
        success_count = 0
        error_count = 0
        with self.conn.cursor() as cursor:
            for qa in qa_data:
                try:
                    question = self.preprocess_text(qa.get('question', ''))
                    answer = self.preprocess_text(qa.get('answer', ''))
                    if not question or not answer:
                        error_count += 1
                        continue
                    qa_id = self.generate_id(question + answer)
                    question_tokens = ' '.join(self.tokenize_chinese(question))
                    category = qa.get('category', self.categorize_question(question))
                    cursor.execute(
                        "REPLACE INTO qa_data (id, question, answer, category, question_tokens, created_at) VALUES (%s, %s, %s, %s, %s, %s)",
                        (qa_id, question, answer, category, question_tokens, datetime.now())
                    )
                    success_count += 1
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
                    error_count += 1
                    continue
            self.conn.commit()
        print(f"âœ… æˆåŠŸåŠ è½½ {success_count} æ¡æ•°æ®")
        return {
            'success': success_count,
            'error': error_count,
            'success_rate': round(success_count / len(qa_data) * 100, 2) if qa_data else 0
        }
    
    def retrieve_relevant_qa(self, user_question: str, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³é—®ç­”ï¼ˆMySQLç‰ˆï¼‰"""
        try:
            processed_question = self.preprocess_text(user_question)
            if not processed_question:
                return []
            with self.conn.cursor() as cursor:
                cursor.execute('SELECT question, answer, category FROM qa_data')
                all_qa = cursor.fetchall()
            if not all_qa:
                print("âš ï¸ çŸ¥è¯†åº“ä¸ºç©º")
                return []
            similarities = []
            for question, answer, category in all_qa:
                similarity = self.calculate_text_similarity(processed_question, question)
                if similarity > 0.1:
                    similarities.append({
                        'question': question,
                        'answer': answer,
                        'category': category,
                        'similarity': round(similarity, 3)
                    })
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            return similarities[:top_k]
        except Exception as e:
            print(f"âŒ æ£€ç´¢é”™è¯¯: {e}")
            return []
    
    def build_enhanced_prompt(self, user_question: str, relevant_qa: List[Dict]) -> str:
        """æ„å»ºå¢å¼ºprompt"""
        if not relevant_qa:
            return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œåå«'è†ææ™ºæœ'ã€‚è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {user_question}

è¯·æä¾›æœ‰ç”¨çš„å¸®åŠ©ï¼Œå›ç­”ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡200å­—ã€‚å¯ä»¥ä½¿ç”¨"äº²äº²"ç­‰äº²åˆ‡ç§°è°“ä¿æŒå‹å¥½è¯­æ°”ã€‚"""
        
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œåå«'è†ææ™ºæœ'ã€‚è¯·æ ¹æ®ä»¥ä¸‹çŸ¥è¯†åº“ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€å‚è€ƒçŸ¥è¯†åº“ã€‘:
"""
        
        for i, qa in enumerate(relevant_qa, 1):
            prompt += f"\nå‚è€ƒ{i} (ç›¸ä¼¼åº¦{qa['similarity']}):\n"
            prompt += f"Q: {qa['question']}\n"
            prompt += f"A: {qa['answer']}\n"
        
        prompt += f"""
ã€ç”¨æˆ·é—®é¢˜ã€‘: {user_question}

è¯·åŸºäºå‚è€ƒçŸ¥è¯†åº“æä¾›å‡†ç¡®å›ç­”ï¼Œä¿æŒäº²åˆ‡å‹å¥½çš„è¯­æ°”ï¼Œå¯ä½¿ç”¨"äº²äº²"ç­‰ç§°è°“ï¼Œæ§åˆ¶åœ¨150å­—ä»¥å†…ã€‚"""
        
        return prompt
    
    def get_enhanced_response(self, user_question: str) -> Dict:
        """è·å–å¢å¼ºå›å¤ä¿¡æ¯"""
        relevant_qa = self.retrieve_relevant_qa(user_question, top_k=3)
        enhanced_prompt = self.build_enhanced_prompt(user_question, relevant_qa)
        category = self.categorize_question(user_question)
        
        return {
            'enhanced_prompt': enhanced_prompt,
            'relevant_qa': relevant_qa,
            'has_relevant_info': len(relevant_qa) > 0,
            'best_similarity': relevant_qa[0]['similarity'] if relevant_qa else 0,
            'category': category,
            'confidence': min(len(relevant_qa) / 3.0, 1.0)
        }
    
    def get_collection_stats(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ï¼ˆMySQLç‰ˆï¼‰"""
        try:
            with self.conn.cursor() as cursor:
                cursor.execute('SELECT COUNT(*) FROM qa_data')
                total_count = cursor.fetchone()[0]
                cursor.execute('SELECT category, COUNT(*) FROM qa_data GROUP BY category')
                categories = dict(cursor.fetchall())
            return {
                'total_documents': total_count,
                'collection_name': 'mysql_qa_database',
                'categories': categories,
                'status': 'active',
                'db_host': MYSQL_CONFIG['host'],
                'db_name': MYSQL_CONFIG['database']
            }
        except Exception as e:
            return {
                'total_documents': 0,
                'status': 'error',
                'error': str(e)
            }
    
    def search_qa(self, keyword: str, limit: int = 10) -> List[Dict]:
        """æœç´¢é—®ç­”ï¼ˆMySQLç‰ˆï¼‰"""
        try:
            with self.conn.cursor() as cursor:
                cursor.execute(
                    'SELECT question, answer, category FROM qa_data WHERE question LIKE %s OR answer LIKE %s LIMIT %s',
                    (f'%{keyword}%', f'%{keyword}%', limit)
                )
                results = []
                for question, answer, category in cursor.fetchall():
                    results.append({
                        'question': question,
                        'answer': answer,
                        'category': category
                    })
            return results
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {e}")
            return []
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if hasattr(self, 'conn'):
            self.conn.close()

# å…¨å±€å•ä¾‹
_offline_rag_service = None

def get_rag_service():
    """è·å–ç¦»çº¿RAGæœåŠ¡å•ä¾‹"""
    global _offline_rag_service
    if _offline_rag_service is None:
        _offline_rag_service = OfflineRAGService()
    return _offline_rag_service

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•ç¦»çº¿RAGæœåŠ¡...")
    try:
        rag = get_rag_service()
        
        # æµ‹è¯•æ•°æ®
        test_data = [
            {'question': 'æœ‰è´§å—', 'answer': 'äº²äº²ï¼Œèƒ½ä¸‹å•å°±æ˜¯åœ¨å”®æœ‰è´§çš„å“¦'},
            {'question': 'ä»€ä¹ˆæ—¶å€™å‘è´§', 'answer': 'äº²äº²ï¼Œæ‰€æœ‰å•†å“æ‹ä¸‹ä¼šé™†ç»­å‘è´§ï¼Œæœ€è¿Ÿä¸è¶…è¿‡48ä¸ªå°æ—¶'}
        ]
        
        # æµ‹è¯•åŠ è½½æ•°æ®
        result = rag.load_qa_data(test_data)
        print(f"ğŸ“Š åŠ è½½ç»“æœ: {result}")
        
        # æµ‹è¯•æ£€ç´¢
        response = rag.get_enhanced_response('æœ‰æ²¡æœ‰è´§ï¼Ÿ')
        print(f"ğŸ” æ£€ç´¢ç»“æœ: ç›¸å…³é—®ç­” {len(response['relevant_qa'])} æ¡")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = rag.get_collection_stats()
        print(f"ğŸ“ˆ çŸ¥è¯†åº“ç»Ÿè®¡: {stats}")
        
        print("âœ… ç¦»çº¿RAGæœåŠ¡æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()