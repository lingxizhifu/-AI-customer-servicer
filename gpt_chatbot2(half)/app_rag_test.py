"""
å®Œå…¨å…¼å®¹çš„RAGæœåŠ¡ - æ”¯æŒåŸæœ‰çš„å¯¼å…¥æ–¹å¼
"""
import os
import json
import re
import sqlite3
import hashlib
from typing import List, Dict, Tuple
from datetime import datetime
from collections import Counter
import math

class RAGService:
    """RAGæœåŠ¡ - å…¼å®¹åŸæœ‰æ¥å£"""
    
    def __init__(self, collection_name="customer_service_qa"):
        self.collection_name = collection_name
        self.db_path = "./rag_data.db"
        self.init_database()
        print("âœ… RAGæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    
    def init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
            self.conn.execute('''
                CREATE TABLE IF NOT EXISTS qa_data (
                    id TEXT PRIMARY KEY,
                    question TEXT NOT NULL,
                    answer TEXT NOT NULL,
                    category TEXT,
                    question_tokens TEXT,
                    created_at TEXT
                )
            ''')
            self.conn.commit()
            print(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text.strip())
        return text
    
    def simple_tokenize(self, text: str) -> List[str]:
        """ç®€å•åˆ†è¯"""
        # å°è¯•å¯¼å…¥jiebaï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€å•åˆ†å‰²
        try:
            import jieba
            words = list(jieba.cut(text))
            stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'æœ‰', 'æ— ', 'ä¸', 'ä¹Ÿ', 'éƒ½', 'å¾ˆ', 'å°±', 'è¦', 'å¯ä»¥', 'èƒ½å¤Ÿ', 'ä¸€ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'æ—¶å€™'}
            filtered_words = [w.strip() for w in words if len(w.strip()) > 1 and w.strip() not in stop_words]
            return filtered_words
        except ImportError:
            # é™çº§åˆ°ç®€å•åˆ†å‰²
            return self.fallback_tokenize(text)
    
    def fallback_tokenize(self, text: str) -> List[str]:
        """é™çº§åˆ†è¯æ–¹æ¡ˆ"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        words = text.split()
        return [w for w in words if len(w) > 1]
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ç›¸ä¼¼åº¦"""
        try:
            text1_clean = self.preprocess_text(text1.lower())
            text2_clean = self.preprocess_text(text2.lower())
            
            if not text1_clean or not text2_clean:
                return 0.0
            
            tokens1 = self.simple_tokenize(text1_clean)
            tokens2 = self.simple_tokenize(text2_clean)
            
            if not tokens1 or not tokens2:
                return self.character_similarity(text1_clean, text2_clean)
            
            # è®¡ç®—è¯æ±‡äº¤é›†
            set1 = set(tokens1)
            set2 = set(tokens2)
            
            intersection = set1 & set2
            union = set1 | set2
            
            if not union:
                return 0.0
            
            # Jaccardç›¸ä¼¼åº¦
            jaccard = len(intersection) / len(union)
            
            # å­—ç¬¦çº§ç›¸ä¼¼åº¦ä½œä¸ºè¡¥å……
            char_sim = self.character_similarity(text1_clean, text2_clean)
            
            # ç»„åˆç›¸ä¼¼åº¦
            return min(jaccard * 0.7 + char_sim * 0.3, 1.0)
            
        except Exception:
            return self.character_similarity(text1, text2)
    
    def character_similarity(self, text1: str, text2: str) -> float:
        """å­—ç¬¦çº§ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        
        # ç®€å•çš„å­—ç¬¦é‡å åº¦
        chars1 = set(text1)
        chars2 = set(text2)
        
        intersection = chars1 & chars2
        union = chars1 | chars2
        
        return len(intersection) / len(union) if union else 0.0
    
    def categorize_question(self, question: str) -> str:
        """é—®é¢˜åˆ†ç±»"""
        categories = {
            'åº“å­˜æŸ¥è¯¢': ['è´§', 'åº“å­˜', 'æœ‰æ²¡æœ‰', 'ç°è´§', 'æœ‰è´§'],
            'ç‰©æµé…é€': ['å‘è´§', 'å¿«é€’', 'ç‰©æµ', 'åˆ°è´§', 'é…é€', 'ä»€ä¹ˆæ—¶å€™'],
            'ä»·æ ¼å’¨è¯¢': ['ä¾¿å®œ', 'ä»·æ ¼', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 'å¤šå°‘é’±'],
            'äº§å“è´¨é‡': ['æ­£å“', 'è´¨é‡', 'çœŸå‡', 'å“è´¨'],
            'å”®åæœåŠ¡': ['é€€è´§', 'æ¢è´§', 'é€€æ¬¾', 'å”®å'],
            'è´¦æˆ·æœåŠ¡': ['ä¼šå‘˜', 'ç§¯åˆ†', 'ä¼˜æƒ åˆ¸'],
            'è®¢å•ç®¡ç†': ['è®¢å•', 'å–æ¶ˆ', 'ä¿®æ”¹', 'æŸ¥è¯¢'],
            'äº§å“å’¨è¯¢': ['æˆåˆ†', 'ä½¿ç”¨', 'æ•ˆæœ', 'è¯´æ˜ä¹¦'],
        }
        
        question_lower = question.lower()
        for category, keywords in categories.items():
            if any(keyword in question_lower for keyword in keywords):
                return category
        return 'å…¶ä»–å’¨è¯¢'
    
    def load_qa_data(self, qa_data: List[Dict]) -> Dict:
        """åŠ è½½é—®ç­”æ•°æ®"""
        if not qa_data:
            return {'success': 0, 'error': 0}
        
        print(f"ğŸ”„ å¼€å§‹åŠ è½½ {len(qa_data)} æ¡é—®ç­”æ•°æ®...")
        
        success_count = 0
        error_count = 0
        
        for qa in qa_data:
            try:
                question = self.preprocess_text(qa.get('question', ''))
                answer = self.preprocess_text(qa.get('answer', ''))
                
                if not question or not answer:
                    error_count += 1
                    continue
                
                qa_id = hashlib.md5((question + answer).encode('utf-8')).hexdigest()[:16]
                category = qa.get('category', self.categorize_question(question))
                
                self.conn.execute('''
                    INSERT OR REPLACE INTO qa_data 
                    (id, question, answer, category, question_tokens, created_at)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (qa_id, question, answer, category, '', datetime.now().isoformat()))
                
                success_count += 1
                
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
                error_count += 1
        
        self.conn.commit()
        print(f"âœ… æˆåŠŸåŠ è½½ {success_count} æ¡æ•°æ®")
        
        return {
            'success': success_count,
            'error': error_count,
            'success_rate': round(success_count / len(qa_data) * 100, 2) if qa_data else 0
        }
    
    def retrieve_relevant_qa(self, user_question: str, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³é—®ç­”"""
        try:
            processed_question = self.preprocess_text(user_question)
            if not processed_question:
                return []
            
            cursor = self.conn.execute('SELECT question, answer, category FROM qa_data')
            all_qa = cursor.fetchall()
            
            if not all_qa:
                return []
            
            similarities = []
            for question, answer, category in all_qa:
                similarity = self.calculate_similarity(processed_question, question)
                if similarity > 0.1:
                    similarities.append({
                        'question': question,
                        'answer': answer,
                        'category': category,
                        'similarity': round(similarity, 3)
                    })
            
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            return similarities[:top_k]
            
        except Exception as e:
            print(f"âŒ æ£€ç´¢é”™è¯¯: {e}")
            return []
    
    def build_enhanced_prompt(self, user_question: str, relevant_qa: List[Dict]) -> str:
        """æ„å»ºå¢å¼ºprompt"""
        if not relevant_qa:
            return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œåå«'è†ææ™ºæœ'ã€‚è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {user_question}

è¯·æä¾›æœ‰ç”¨çš„å¸®åŠ©ï¼Œå›ç­”ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡200å­—ã€‚"""
        
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œåå«'è†ææ™ºæœ'ã€‚è¯·æ ¹æ®ä»¥ä¸‹çŸ¥è¯†åº“ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€å‚è€ƒçŸ¥è¯†åº“ã€‘:
"""
        
        for i, qa in enumerate(relevant_qa, 1):
            prompt += f"\nå‚è€ƒ{i}: Q: {qa['question']}\nA: {qa['answer']}\n"
        
        prompt += f"""
ã€ç”¨æˆ·é—®é¢˜ã€‘: {user_question}

è¯·åŸºäºå‚è€ƒçŸ¥è¯†åº“æä¾›å‡†ç¡®å›ç­”ï¼Œä¿æŒå‹å¥½è¯­æ°”ï¼Œæ§åˆ¶åœ¨150å­—ä»¥å†…ã€‚"""
        
        return prompt
    
    def get_enhanced_response(self, user_question: str) -> Dict:
        """è·å–å¢å¼ºå›å¤ä¿¡æ¯"""
        relevant_qa = self.retrieve_relevant_qa(user_question, top_k=3)
        enhanced_prompt = self.build_enhanced_prompt(user_question, relevant_qa)
        category = self.categorize_question(user_question)
        
        return {
            'enhanced_prompt': enhanced_prompt,
            'relevant_qa': relevant_qa,
            'has_relevant_info': len(relevant_qa) > 0,
            'best_similarity': relevant_qa[0]['similarity'] if relevant_qa else 0,
            'category': category,
            'confidence': min(len(relevant_qa) / 3.0, 1.0)
        }
    
    def get_collection_stats(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡"""
        try:
            cursor = self.conn.execute('SELECT COUNT(*) FROM qa_data')
            total_count = cursor.fetchone()[0]
            
            cursor = self.conn.execute('SELECT category, COUNT(*) FROM qa_data GROUP BY category')
            categories = dict(cursor.fetchall())
            
            return {
                'total_documents': total_count,
                'collection_name': self.collection_name,
                'categories': categories,
                'status': 'active'
            }
        except Exception as e:
            return {
                'total_documents': 0,
                'status': 'error',
                'error': str(e)
            }

# å…¨å±€å•ä¾‹
_rag_service = None

def get_rag_service():
    """è·å–RAGæœåŠ¡å•ä¾‹"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•RAGæœåŠ¡...")
    try:
        rag = RAGService()
        
        test_data = [
            {'question': 'æœ‰è´§å—', 'answer': 'äº²äº²ï¼Œèƒ½ä¸‹å•å°±æ˜¯åœ¨å”®æœ‰è´§çš„å“¦'},
            {'question': 'ä»€ä¹ˆæ—¶å€™å‘è´§', 'answer': 'äº²äº²ï¼Œæ‰€æœ‰å•†å“æ‹ä¸‹ä¼šé™†ç»­å‘è´§ï¼Œæœ€è¿Ÿä¸è¶…è¿‡48ä¸ªå°æ—¶'}
        ]
        
        result = rag.load_qa_data(test_data)
        print(f"ğŸ“Š åŠ è½½ç»“æœ: {result}")
        
        response = rag.get_enhanced_response('æœ‰æ²¡æœ‰è´§ï¼Ÿ')
        print(f"ğŸ” æ£€ç´¢ç»“æœ: ç›¸å…³é—®ç­” {len(response['relevant_qa'])} æ¡")
        
        stats = rag.get_collection_stats()
        print(f"ğŸ“ˆ çŸ¥è¯†åº“ç»Ÿè®¡: {stats}")
        
        print("âœ… RAGæœåŠ¡æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()